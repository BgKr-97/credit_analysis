# Автоматизированная система анализа просроченных клиентов

### Цель проекта
- Спроектировать DWH с таблицами Loans (ID, клиент, сумма, срок, статус) и Clients (ID, имя).
- Реализовать ETL для загрузки JSON, пометить кредиты как просроченные (дата > срока), вычислить процент просрочки.
- Создать BI-витрину: график доли просрочек по месяцам, фильтр по сумме.
- Провести тестирование на 10 записях.
- Предложить оптимизацию процесса уведомления клиентов.
- Усложнение: вычисления и тестирование.

### Исходные данные проекта

- таблица Clients: содержит подробную информацию о каждом клиенте;
- таблица Loans: содержит информацию о кредитах клиентов;
- таблица Payments: содержит историчную информацию о платежах клиентов.

### Хранилище данных
Система предусматривает хранение данных в едином хранилище данных (ЕХД), разделенном на несколько слоев:
- слой staging - здесь хранятся «сырые» загрузки из JSON-файлов, хранящих информацию о клиентах, их кредитах и истории платежей;
- слой core - здесь хранятся данные, преобразованные со слоя staging без затирания;
- слой mart - здесь формируется витрина для BI — агрегированные и подготовленные для отчёта данные.

# Пример финального предоставления отчета через Power BI
![Image](https://github.com/user-attachments/assets/390e630a-0d2f-450e-84ab-50603d88a4e9)

# Подготовительные операции

## 1. Копируем содержимое проекта себе в рабочую директорию
``` Python
git clone <метод копирования>
```

## 2. Устанавливаем библиотеки
``` Python
pip install -r requirements.txt
```

## 3. Для хранения переменных окружения создаем файл .env
В файл .env записываем пути до файлов с исходными данными, а также параметры для подключения к вашей БД (базе данных).

**Обозначения:**

`START_LOAN_DATE` - минимальная дата открытия кредита

`RAW_DIR` - папка для хранения "сырого" источника данных

`SPLIT_DIR` - папка для хранения "сырого" источника данных, разделенного на несколько частей

``` txt
# Подключение к БД
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASS=***
DB_NAME=postgres

# Папки с данными
RAW_DIR=data_generation/raw_files
SPLIT_DIR=data_generation/raw_split_files

# Настройки генерации данных
START_LOAN_DATE=2010-01-01
```

## 4. Создание структуры DWH:
В проекте реализован автоматический механизм создания структуры БД - через исполнение SQL-скриптов. Вызов данных скриптов автоматизирован - он осуществляется через обработку аргументов командной строки с помощью `argparse`. То есть, чтобы запустить процесс инициализации БД, достаточно выполнить соответствующую команду:
``` Python
python main.py shema
```
В ответ на этот аргумент код сам найдет и выполнит нужные SQL-файлы, создав все схемы и таблицы в нужном порядке.

### Генерация/загрузка/преобразование данных
## 1. Генерация данных
В проекте реализована инкрементная загрузка данных в БД, а не полным дампом — это стандартный и осознанный подход в продакшн-системах.
1. Указав параметры для генерации данных, команда 
``` Python
python main.py generate
```
сгенерирует "сырые" данные в виде трех JSON-файлов (clients.json, loans.json и payments.json) и сохранит их в указанную в файле `.env` папку. По умолчанию генерируется 20 клиентов.

Задав дополнительный параметр `--num-clients`, команда `generate` сгенерирует необходимое количество клиентов. 
Например, команда
``` Python
python main.py generate --num-clients 100
```
сгенерирует JSON-файлы, в которых будет учтено 100 клиентов и соответствующие им операции.

2. Команда 
``` Python
python main.py split
```
разделит существующие JSON-файлы на n частей (по умолчанию `n = 5`), и сохранит их в указанную в файле `.env` папку.
Разделение происходит *кумулятивно*, то есть каждый последующий файл содержит в себе информацию предыдущей части. 
Это необходимо для реализации метода UPSERT, предназначенного для поддержания целостности данных и избежания дубликатов записей.

Задав дополнительный параметр `--parts`, команда `split` разделит исходные JSON-файлы на необходимое количество частей. Например, команда
``` Python
python main.py split --parts 10
```
разделит исходные JSON-файлы на 10 частей. Разделенные JSON-файлы будут храниться в указанной в файле `.env` папке с указанием номера части.

## 2. Загрузка данных
Перевод данных с одного слоя на другой осуществляется посредством автоматического запуска ETL процессов в целях упрощения взаимодействия с системой. 
Вызов команды `load` 
``` Python
python main.py load --part N
```
инкрементально загружает одну часть данных (файлы clients_N.json, loans_N.json и payments_N.json, где N - номер части) сразу во все слои БД:
- слой staging: сюда данные попадают с сохранением своего первоначального вида и с указанием timestamp записи, а также источника получения данных. Уникальность записей проверяется по отдельным полям;
- слой core: преобразованные со слоя staging данные попадают сюда без затирания. При загрузке данных на этот слой добавляется булево поле `status`, показывающее просроченную операцию клиентов по их кредитам. Уникальность записей проверяется по отдельным полям;
- слой mart: сюда данные попадают со слоя core в денормализованном виде. Заранее подготовленное представление (VIEW) на данном слое служит источником для демонстрации данных в Power BI.

Задав дополнительный параметр `--all`, команда `load` загрузит в БД сразу все части данных:
``` Python
python main.py load --all
```

**Предпочтительный** способ загрузки данных - инкрементный (по частям). Данный способ дает возможность ощутить периодическое обновление данных, как в реальных проектах.

# Пример процесса загрузки данных



# Демонстрация данных
Демонстрация данных производится на основании PowerBI из таблицы big_transactions, находящийся на слое data_mart:

Связываем PowerBi с нашей БД с таблицей big_transactions
Загружаем данные
Пример отчета находится в файле витрина.pbix

Цели проекта
Код написан в учебных целях — это практическое задание в курсе "Системный аналитик" от Лиги Цифровых Экономик.





